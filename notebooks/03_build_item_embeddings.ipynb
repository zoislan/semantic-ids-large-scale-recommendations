{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f178a561-c9e4-46f4-956f-7b9e4f69a455",
   "metadata": {},
   "source": [
    "# 03 — Build Item Embeddings (inputs for Semantic IDs)\n",
    "\n",
    "**Goal.** Convert each item’s text (`description`) into a fixed 384‑dimensional embedding\n",
    "using `sentence-transformers/all-MiniLM-L6-v2`, saving sharded `.pt` files (fp16, L2‑normalized).\n",
    "These continuous vectors will be quantized into **Semantic IDs** in the next notebook.\n",
    "\n",
    "**Why this step?** The paper’s idea is to replace raw IDs with *semantic* identifiers that generalize.\n",
    "This notebook produces the semantic *content* vectors we’ll discretize next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91f593-caef-47e2-8906-4d18df890da1",
   "metadata": {},
   "source": [
    "### 1. Imports, environment, and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c3fdf5-2fd0-4035-a27a-ba11c51c58ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: data/processed/books/items.parquet\n",
      "Out  : data/embeddings/items_miniLM\n"
     ]
    }
   ],
   "source": [
    "# --- Imports\n",
    "import os, gc, json, time, glob, math, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Env knobs (to avoid ipywidgets progress errors and tokenizer thread spam)\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Reproducability\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Paths \n",
    "ITEMS_FP   = \"data/processed/books/items.parquet\"  # from notebook 2\n",
    "TEXT_COL   = \"description\"                               # column to encode\n",
    "OUT_DIR    = \"data/embeddings/items_miniLM\"        # new output dir\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Items:\", ITEMS_FP)\n",
    "print(\"Out  :\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d62ac-2661-456e-bfdb-835c5c4abdc8",
   "metadata": {},
   "source": [
    "### 2. Config: model + throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b6c57c-0da8-4678-a2f2-c3ba99756433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN: 256 | ARROW_BATCH_ROWS: 200000 | MICRO_BS: 512\n"
     ]
    }
   ],
   "source": [
    "# Model + speed knobs\n",
    "MODEL_ID         = \"sentence-transformers/all-MiniLM-L6-v2\"   # 384-d\n",
    "CACHE_DIR        = \"models/all-MiniLM-L6-v2\"                  # local cache\n",
    "MAX_LEN          = 256                                        # token window\n",
    "ARROW_BATCH_ROWS = 200_000                                    # rows per Arrow batch (feedback every batch)\n",
    "TARGET_MICRO_BS  = 512                                        # items per forward; auto-fallback on OOM\n",
    "DTYPE            = torch.float16                              # fp16 on disk\n",
    "NORM             = True                                       # L2-normalize rows (cosine-ready)\n",
    "USE_MANIFEST     = False\n",
    "MANIFEST_FP      = os.path.join(OUT_DIR, \"manifest.jsonl\")\n",
    "\n",
    "if USE_MANIFEST:\n",
    "    open(MANIFEST_FP, \"w\").close()\n",
    "\n",
    "print(\"MAX_LEN:\", MAX_LEN, \"| ARROW_BATCH_ROWS:\", ARROW_BATCH_ROWS, \"| MICRO_BS:\", TARGET_MICRO_BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e43e74-4b00-432f-925d-06cdeff605bd",
   "metadata": {},
   "source": [
    "### 3. Device and model load (+ test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234deac4-b7de-49b6-9118-1e9eb9d5ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Model ready. Embedding dim: 384\n"
     ]
    }
   ],
   "source": [
    "# Device selection (MPS)\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Load tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "model     = SentenceTransformer(MODEL_ID, cache_folder=CACHE_DIR, device=DEVICE)\n",
    "model.max_seq_length = MAX_LEN\n",
    "\n",
    "# test \n",
    "_ = model.encode([\"hello world\"], batch_size=4, convert_to_tensor=True, show_progress_bar=False, device=DEVICE)\n",
    "print(\"Model ready. Embedding dim:\", model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d5a87-0bea-4b94-b8b5-80d768d290f6",
   "metadata": {},
   "source": [
    "### 4. Resume helper: which shards already exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e33c1a-0a91-4d35-8a39-33f2b498a051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing shards: 13\n",
      "Example: shard_batch_000012.pt\n"
     ]
    }
   ],
   "source": [
    "# We resume by skipping shard files already on disk.\n",
    "existing = sorted(glob.glob(os.path.join(OUT_DIR, \"shard_batch_*.pt\")))\n",
    "print(\"Existing shards:\", len(existing))\n",
    "if len(existing) > 0:\n",
    "    print(\"Example:\", os.path.basename(existing[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24f2b2-24e6-4a01-a58b-0f4afcb15eb7",
   "metadata": {},
   "source": [
    "### 5. Encoders: simple (default) and optional “full‑coverage” chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437217dd-2776-487a-b429-9bccf0a501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_texts_simple(texts: List[str], init_bs=TARGET_MICRO_BS) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode list[str] -> [N, d] tensor on CPU (fp16), with L2-norm if NORM=True.\n",
    "    Auto‑fallback halves batch size on OOM.\n",
    "    \"\"\"\n",
    "    d = model.get_sentence_embedding_dimension()\n",
    "    if not texts:\n",
    "        return torch.empty((0, d), dtype=DTYPE)\n",
    "\n",
    "    out = []\n",
    "    bs  = init_bs\n",
    "    i   = 0\n",
    "    while i < len(texts):\n",
    "        step = min(bs, len(texts) - i)\n",
    "        try:\n",
    "            emb = model.encode(\n",
    "                texts[i:i+step],\n",
    "                batch_size=step,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                device=DEVICE,\n",
    "                normalize_embeddings=False, \n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Auto OOM fallback on MPS/CUDA\n",
    "            if \"out of memory\" in str(e).lower() and bs > 16:\n",
    "                if DEVICE == \"mps\": torch.mps.empty_cache()\n",
    "                bs //= 2\n",
    "                print(f\"OOM fallback -> micro-batch {bs}\")\n",
    "                continue\n",
    "            raise\n",
    "        if NORM:\n",
    "            emb = F.normalize(emb, p=2, dim=1)\n",
    "        out.append(emb.to(\"cpu\").to(DTYPE))\n",
    "        i += step\n",
    "        del emb\n",
    "        if DEVICE == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "    return torch.cat(out, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d1c61-2fcb-4537-b358-10d918f6214c",
   "metadata": {},
   "source": [
    "### 6. Streaming loop: read → encode → save shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7079614f-b69e-4dc8-96d1-ba3585a837ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-groups: 3\n",
      "[batch 000000] skipping (exists) → shard_batch_000000.pt\n",
      "[batch 000001] skipping (exists) → shard_batch_000001.pt\n",
      "[batch 000002] skipping (exists) → shard_batch_000002.pt\n",
      "[batch 000003] skipping (exists) → shard_batch_000003.pt\n",
      "[batch 000004] skipping (exists) → shard_batch_000004.pt\n",
      "[batch 000005] skipping (exists) → shard_batch_000005.pt\n",
      "[batch 000006] skipping (exists) → shard_batch_000006.pt\n",
      "[batch 000007] skipping (exists) → shard_batch_000007.pt\n",
      "[batch 000008] skipping (exists) → shard_batch_000008.pt\n",
      "[batch 000009] skipping (exists) → shard_batch_000009.pt\n",
      "[batch 000010] skipping (exists) → shard_batch_000010.pt\n",
      "[batch 000011] skipping (exists) → shard_batch_000011.pt\n",
      "[batch 000012] skipping (exists) → shard_batch_000012.pt\n",
      "[batch 000013] rows=200,000 → shard_batch_000013.pt | 197 items/s | total=2,800,000\n",
      "[batch 000014] rows=200,000 → shard_batch_000014.pt | 198 items/s | total=3,000,000\n",
      "[batch 000015] rows=47,489 → shard_batch_000015.pt | 195 items/s | total=3,047,489\n",
      "Done. Batches: 16 | items: 3,047,489 | time: 37.8 min\n"
     ]
    }
   ],
   "source": [
    "pf = pq.ParquetFile(ITEMS_FP)\n",
    "print(\"Row-groups:\", pf.num_row_groups)\n",
    "\n",
    "# For \"resume\": build a set of shard basenames that already exist\n",
    "existing_set = {os.path.basename(p) for p in existing}\n",
    "\n",
    "total_items = 0\n",
    "batch_idx   = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for batch in pf.iter_batches(columns=[\"item_id\", TEXT_COL], batch_size=ARROW_BATCH_ROWS):\n",
    "    # Convert only needed columns to Python lists (avoid building a giant DataFrame)\n",
    "    ids   = batch.column(0).to_pylist()\n",
    "    texts = batch.column(1).to_pylist()\n",
    "\n",
    "    # Drop empties fast\n",
    "    keep = [i for i, t in enumerate(texts) if t is not None and t != \"\"]\n",
    "    if len(keep) != len(texts):\n",
    "        ids   = [ids[i]   for i in keep]\n",
    "        texts = [texts[i] for i in keep]\n",
    "\n",
    "    shard_name = f\"shard_batch_{batch_idx:06d}.pt\"\n",
    "    shard_path = os.path.join(OUT_DIR, shard_name)\n",
    "\n",
    "    # Resume: skip if shard already exists\n",
    "    if shard_name in existing_set:\n",
    "        print(f\"[batch {batch_idx:06d}] skipping (exists) → {shard_name}\")\n",
    "        total_items += len(ids)\n",
    "        batch_idx   += 1\n",
    "        # clean up\n",
    "        del ids, texts, batch\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    if not texts:\n",
    "        print(f\"[batch {batch_idx:06d}] empty texts; skipping\")\n",
    "        batch_idx += 1\n",
    "        del ids, texts, batch\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    t_batch = time.time()\n",
    "    # Encode\n",
    "    E = encode_texts_simple(texts, init_bs=TARGET_MICRO_BS)   # [Nb, 384] on CPU, fp16\n",
    "\n",
    "    # Save shard\n",
    "    torch.save({\"item_id\": ids[:len(E)], \"embeddings\": E}, shard_path)\n",
    "\n",
    "    if USE_MANIFEST:\n",
    "        with open(MANIFEST_FP, \"a\") as mf:\n",
    "            mf.write(json.dumps({\"batch\": batch_idx, \"n\": len(E), \"path\": shard_name}) + \"\\n\")\n",
    "\n",
    "    # progress\n",
    "    total_items += len(E)\n",
    "    dt = time.time() - t_batch\n",
    "    ips = int(len(E) / max(dt, 1e-6))\n",
    "    print(f\"[batch {batch_idx:06d}] rows={len(E):,} → {shard_name} | {ips} items/s | total={total_items:,}\")\n",
    "\n",
    "    # cleanup\n",
    "    del ids, texts, E, batch\n",
    "    gc.collect()\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    batch_idx += 1\n",
    "\n",
    "print(f\"Done. Batches: {batch_idx:,} | items: {total_items:,} | time: {(time.time()-t0)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1173088-767a-44c7-a526-a5dc2e0a886e",
   "metadata": {},
   "source": [
    "### 7. Concatenate shards → one matrix & row-normalize (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d76a5d88-9875-4050-94cb-95a88706991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: (3047489, 384) float16\n"
     ]
    }
   ],
   "source": [
    "paths_pt = sorted(glob.glob(os.path.join(OUT_DIR, \"shard_batch_*.pt\")))\n",
    "assert paths_pt, \"No shards found.\"\n",
    "\n",
    "ids_chunks, vec_chunks = [], []\n",
    "for p in paths_pt:\n",
    "    blob = torch.load(p, map_location=\"cpu\")\n",
    "    ids_chunks.append(np.asarray(blob[\"item_id\"], dtype=object))\n",
    "    # keep in float16 from disk, cast to float32 only for stable normalization math\n",
    "    v = blob[\"embeddings\"].to(torch.float16).cpu().numpy().astype(np.float32, copy=False)\n",
    "    vec_chunks.append(v)\n",
    "\n",
    "item_ids = np.concatenate(ids_chunks)\n",
    "X = np.vstack(vec_chunks)                      # float32 now\n",
    "# row-wise L2 normalize (cosine-ready)\n",
    "norm = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "norm[norm == 0] = 1.0\n",
    "X = (X / norm).astype(np.float16, copy=False)  # back to fp16 for storage\n",
    "\n",
    "np.save(os.path.join(OUT_DIR, \"item_ids.npy\"), item_ids)\n",
    "np.save(os.path.join(OUT_DIR, \"item_embeds.npy\"), X)\n",
    "print(\"Saved:\", X.shape, X.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955aeb9-a5fb-4ca1-bb48-ef35589bd5fb",
   "metadata": {},
   "source": [
    "### 8. Sanity checks: shape, dtype, norms, quick NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec58b578-5246-42ae-80cb-3ad19b764690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: (3047489,) object | E: (3047489, 384) float16\n",
      "norms -> mean: 0.99999994 std: 1.8474228e-05 min: 0.99991024 max: 1.0000939\n",
      "q0 → [('0701169850', 1.0000230073928833), ('B008DM2LQ8', 0.465138703584671), ('1441599258', 0.45543068647384644), ('3548241107', 0.4497717320919037), ('1133307299', 0.4491029381752014)]\n",
      "q1 → [('0316185361', 1.0000216960906982), ('0786477946', 0.4279870390892029), ('1501121960', 0.4205887019634247), ('0330361120', 0.3991989195346832), ('1585426628', 0.3881700336933136)]\n",
      "q2 → [('0545425573', 1.0000718832015991), ('0670015547', 0.5404351353645325), ('1087719887', 0.5104107856750488), ('B007HXFBDE', 0.4974365532398224), ('0340875577', 0.49628835916519165)]\n",
      "q3 → [('B00KFOP3RG', 1.0000190734863281), ('B01F0OPEB0', 0.4912773370742798), ('0984903054', 0.49120232462882996), ('1643136135', 0.4862109124660492), ('B09WFB2Z94', 0.4673328995704651)]\n",
      "q4 → [('B09PHG4FQ8', 1.0000277757644653), ('0134485351', 0.6188583970069885), ('B08C4FTJF5', 0.5452044010162354), ('0766015491', 0.5338360667228699), ('1556527667', 0.5329293608665466)]\n"
     ]
    }
   ],
   "source": [
    "# reload to be sure we’re reading what we wrote\n",
    "ids = np.load(OUT_DIR / \"item_ids.npy\", allow_pickle=True)\n",
    "E   = np.load(OUT_DIR / \"item_embeds.npy\")\n",
    "\n",
    "print(\"IDs:\", ids.shape, ids.dtype, \"| E:\", E.shape, E.dtype)\n",
    "\n",
    "# norms (should be 1.0 mean/std)\n",
    "row_norms = np.linalg.norm(E.astype(np.float32), axis=1)\n",
    "print(\"norms -> mean:\", row_norms.mean(), \"std:\", row_norms.std(), \"min:\", row_norms.min(), \"max:\", row_norms.max())\n",
    "\n",
    "# quick NN\n",
    "m = min(2000, E.shape[0])\n",
    "Q = E[:5].astype(np.float32, copy=False)\n",
    "C = E[:m].astype(np.float32, copy=False)\n",
    "S = Q @ C.T                      # cosine since rows are unit\n",
    "k = 5\n",
    "idx_unsorted = np.argpartition(-S, k-1, axis=1)[:, :k]\n",
    "\n",
    "# sort within the top-k\n",
    "row_sorted = np.take_along_axis(\n",
    "    np.argsort(-np.take_along_axis(S, idx_unsorted, axis=1), axis=1),\n",
    "    np.arange(k)[None, :],\n",
    "    axis=1\n",
    ")\n",
    "topk_idx = np.take_along_axis(idx_unsorted, row_sorted, axis=1)\n",
    "topk_scores = np.take_along_axis(S, topk_idx, axis=1)\n",
    "\n",
    "# map to item_ids\n",
    "for qi in range(Q.shape[0]):\n",
    "    nbrs = [(ids[j], float(topk_scores[qi, c])) for c, j in enumerate(topk_idx[qi])]\n",
    "    print(f\"q{qi} →\", nbrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62017e5-1475-4ff7-be2c-be859102b428",
   "metadata": {},
   "source": [
    "### 9. Metadata Snapshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc49edd9-29da-4116-9356-e46defc6cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/embeddings/items_miniLM/stats.json\n"
     ]
    }
   ],
   "source": [
    "meta = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"cache_dir\": CACHE_DIR,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"arrow_batch_rows\": ARROW_BATCH_ROWS,\n",
    "    \"target_micro_bs\": TARGET_MICRO_BS,\n",
    "    \"dtype\": \"float16\",\n",
    "    \"norm_after_concat\": True,\n",
    "    \"device\": DEVICE,\n",
    "    \"items_file\": ITEMS_FP,\n",
    "    \"text_col\": TEXT_COL,\n",
    "    \"out_final\": str(OUT_DIR),\n",
    "}\n",
    "with open(OUT_DIR / \"stats.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved:\", OUT_DIR / \"stats.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis M4",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
