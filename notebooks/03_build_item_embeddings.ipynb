{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f178a561-c9e4-46f4-956f-7b9e4f69a455",
   "metadata": {},
   "source": [
    "# 03 — Build Item Embeddings (inputs for Semantic IDs)\n",
    "\n",
    "**Goal.** Convert each item’s text (`description`) into a fixed 384‑dimensional embedding\n",
    "using `sentence-transformers/all-MiniLM-L6-v2`, saving sharded `.pt` files (fp16, L2‑normalized).\n",
    "These continuous vectors will be quantized into **Semantic IDs** in the next notebook.\n",
    "\n",
    "**Why this step?** The paper’s idea is to replace raw IDs with *semantic* identifiers that generalize.\n",
    "This notebook produces the semantic *content* vectors we’ll discretize next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91f593-caef-47e2-8906-4d18df890da1",
   "metadata": {},
   "source": [
    "### 1. Imports, environment, and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c3fdf5-2fd0-4035-a27a-ba11c51c58ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: data/processed/books/items.parquet\n",
      "Out  : data/embeddings/items_miniLM\n"
     ]
    }
   ],
   "source": [
    "# --- Imports\n",
    "import os, gc, json, time, glob, math, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Env knobs (avoid ipywidgets progress errors and tokenizer thread spam)\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- Repro\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- Paths (adjust if needed)\n",
    "ITEMS_FP   = \"data/processed/books/items.parquet\"  # from notebook 2\n",
    "TEXT_COL   = \"description\"                               # column to encode\n",
    "OUT_DIR    = \"data/embeddings/items_miniLM\"        # new output dir\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Items:\", ITEMS_FP)\n",
    "print(\"Out  :\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d62ac-2661-456e-bfdb-835c5c4abdc8",
   "metadata": {},
   "source": [
    "### 2. Config: model + throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b6c57c-0da8-4678-a2f2-c3ba99756433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN: 256 | ARROW_BATCH_ROWS: 200000 | MICRO_BS: 512\n"
     ]
    }
   ],
   "source": [
    "# --- Model + speed knobs\n",
    "MODEL_ID         = \"sentence-transformers/all-MiniLM-L6-v2\"   # 384-d\n",
    "CACHE_DIR        = \"models/all-MiniLM-L6-v2\"                  # local cache\n",
    "MAX_LEN          = 256                                        # token window\n",
    "ARROW_BATCH_ROWS = 200_000                                    # rows per Arrow batch (feedback every batch)\n",
    "TARGET_MICRO_BS  = 512                                        # items per forward; auto-fallback on OOM\n",
    "DTYPE            = torch.float16                              # fp16 on disk\n",
    "NORM             = True                                       # L2-normalize rows (cosine-ready)\n",
    "USE_MANIFEST     = False                                      # usually unnecessary; shards are source of truth\n",
    "MANIFEST_FP      = os.path.join(OUT_DIR, \"manifest.jsonl\")\n",
    "\n",
    "if USE_MANIFEST:\n",
    "    open(MANIFEST_FP, \"w\").close()\n",
    "\n",
    "print(\"MAX_LEN:\", MAX_LEN, \"| ARROW_BATCH_ROWS:\", ARROW_BATCH_ROWS, \"| MICRO_BS:\", TARGET_MICRO_BS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e43e74-4b00-432f-925d-06cdeff605bd",
   "metadata": {},
   "source": [
    "### 3. Device and model load (+ test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234deac4-b7de-49b6-9118-1e9eb9d5ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Model ready. Embedding dim: 384\n"
     ]
    }
   ],
   "source": [
    "# --- Device selection (MPS on Apple Silicon)\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# --- Load tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "model     = SentenceTransformer(MODEL_ID, cache_folder=CACHE_DIR, device=DEVICE)\n",
    "model.max_seq_length = MAX_LEN\n",
    "\n",
    "# --- Sanity smoke test (also warms up MPS)\n",
    "_ = model.encode([\"hello world\"], batch_size=4, convert_to_tensor=True, show_progress_bar=False, device=DEVICE)\n",
    "print(\"Model ready. Embedding dim:\", model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d5a87-0bea-4b94-b8b5-80d768d290f6",
   "metadata": {},
   "source": [
    "### 4. Resume helper: which shards already exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e33c1a-0a91-4d35-8a39-33f2b498a051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing shards: 17\n",
      "Example: shard_batch_000016.pt\n"
     ]
    }
   ],
   "source": [
    "# We resume by skipping shard files already on disk.\n",
    "existing = sorted(glob.glob(os.path.join(OUT_DIR, \"shard_batch_*.pt\")))\n",
    "print(\"Existing shards:\", len(existing))\n",
    "if len(existing) > 0:\n",
    "    print(\"Example:\", os.path.basename(existing[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24f2b2-24e6-4a01-a58b-0f4afcb15eb7",
   "metadata": {},
   "source": [
    "### 5. Encoders: simple (default) and optional “full‑coverage” chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437217dd-2776-487a-b429-9bccf0a501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_texts_simple(texts: List[str], init_bs=TARGET_MICRO_BS) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode list[str] -> [N, d] tensor on CPU (fp16), with L2-norm if NORM=True.\n",
    "    Auto‑fallback halves batch size on OOM.\n",
    "    \"\"\"\n",
    "    d = model.get_sentence_embedding_dimension()\n",
    "    if not texts:\n",
    "        return torch.empty((0, d), dtype=DTYPE)\n",
    "\n",
    "    out = []\n",
    "    bs  = init_bs\n",
    "    i   = 0\n",
    "    while i < len(texts):\n",
    "        step = min(bs, len(texts) - i)\n",
    "        try:\n",
    "            emb = model.encode(\n",
    "                texts[i:i+step],\n",
    "                batch_size=step,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                device=DEVICE,\n",
    "                normalize_embeddings=False,   # we will normalize ourselves\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Auto OOM fallback on MPS/CUDA\n",
    "            if \"out of memory\" in str(e).lower() and bs > 16:\n",
    "                if DEVICE == \"mps\": torch.mps.empty_cache()\n",
    "                bs //= 2\n",
    "                print(f\"OOM fallback -> micro-batch {bs}\")\n",
    "                continue\n",
    "            raise\n",
    "        if NORM:\n",
    "            emb = F.normalize(emb, p=2, dim=1)\n",
    "        out.append(emb.to(\"cpu\").to(DTYPE))\n",
    "        i += step\n",
    "        del emb\n",
    "        if DEVICE == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "    return torch.cat(out, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d1c61-2fcb-4537-b358-10d918f6214c",
   "metadata": {},
   "source": [
    "### 6. Streaming loop: read → encode → save shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7079614f-b69e-4dc8-96d1-ba3585a837ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-groups: 4\n",
      "[batch 000000] skipping (exists) → shard_batch_000000.pt\n",
      "[batch 000001] skipping (exists) → shard_batch_000001.pt\n",
      "[batch 000002] skipping (exists) → shard_batch_000002.pt\n",
      "[batch 000003] skipping (exists) → shard_batch_000003.pt\n",
      "[batch 000004] skipping (exists) → shard_batch_000004.pt\n",
      "[batch 000005] skipping (exists) → shard_batch_000005.pt\n",
      "[batch 000006] skipping (exists) → shard_batch_000006.pt\n",
      "[batch 000007] skipping (exists) → shard_batch_000007.pt\n",
      "[batch 000008] skipping (exists) → shard_batch_000008.pt\n",
      "[batch 000009] skipping (exists) → shard_batch_000009.pt\n",
      "[batch 000010] skipping (exists) → shard_batch_000010.pt\n",
      "[batch 000011] skipping (exists) → shard_batch_000011.pt\n",
      "[batch 000012] skipping (exists) → shard_batch_000012.pt\n",
      "[batch 000013] skipping (exists) → shard_batch_000013.pt\n",
      "[batch 000014] skipping (exists) → shard_batch_000014.pt\n",
      "[batch 000015] skipping (exists) → shard_batch_000015.pt\n",
      "[batch 000016] skipping (exists) → shard_batch_000016.pt\n",
      "[batch 000017] rows=200,000 → shard_batch_000017.pt | 199 items/s | total=3,600,000\n",
      "[batch 000018] rows=200,000 → shard_batch_000018.pt | 204 items/s | total=3,800,000\n",
      "[batch 000019] rows=183,264 → shard_batch_000019.pt | 203 items/s | total=3,983,264\n",
      "Done. Batches: 20 | items: 3,983,264 | time: 48.1 min\n"
     ]
    }
   ],
   "source": [
    "pf = pq.ParquetFile(ITEMS_FP)\n",
    "print(\"Row-groups:\", pf.num_row_groups)\n",
    "\n",
    "# For \"resume\": build a set of shard basenames that already exist\n",
    "existing_set = {os.path.basename(p) for p in existing}\n",
    "\n",
    "total_items = 0\n",
    "batch_idx   = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for batch in pf.iter_batches(columns=[\"item_id\", TEXT_COL], batch_size=ARROW_BATCH_ROWS):\n",
    "    # Convert only needed columns to Python lists (avoid building a giant DataFrame)\n",
    "    ids   = batch.column(0).to_pylist()\n",
    "    texts = batch.column(1).to_pylist()\n",
    "\n",
    "    # Drop empties fast\n",
    "    keep = [i for i, t in enumerate(texts) if t is not None and t != \"\"]\n",
    "    if len(keep) != len(texts):\n",
    "        ids   = [ids[i]   for i in keep]\n",
    "        texts = [texts[i] for i in keep]\n",
    "\n",
    "    shard_name = f\"shard_batch_{batch_idx:06d}.pt\"\n",
    "    shard_path = os.path.join(OUT_DIR, shard_name)\n",
    "\n",
    "    # Resume: skip if shard already exists\n",
    "    if shard_name in existing_set:\n",
    "        print(f\"[batch {batch_idx:06d}] skipping (exists) → {shard_name}\")\n",
    "        total_items += len(ids)\n",
    "        batch_idx   += 1\n",
    "        # clean up\n",
    "        del ids, texts, batch\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    if not texts:\n",
    "        print(f\"[batch {batch_idx:06d}] empty texts; skipping\")\n",
    "        batch_idx += 1\n",
    "        del ids, texts, batch\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    t_batch = time.time()\n",
    "    # --- Encode\n",
    "    E = encode_texts_simple(texts, init_bs=TARGET_MICRO_BS)   # [Nb, 384] on CPU, fp16\n",
    "\n",
    "    # --- Save shard\n",
    "    torch.save({\"item_id\": ids[:len(E)], \"embeddings\": E}, shard_path)\n",
    "\n",
    "    # --- Optional manifest record\n",
    "    if USE_MANIFEST:\n",
    "        with open(MANIFEST_FP, \"a\") as mf:\n",
    "            mf.write(json.dumps({\"batch\": batch_idx, \"n\": len(E), \"path\": shard_name}) + \"\\n\")\n",
    "\n",
    "    # progress\n",
    "    total_items += len(E)\n",
    "    dt = time.time() - t_batch\n",
    "    ips = int(len(E) / max(dt, 1e-6))\n",
    "    print(f\"[batch {batch_idx:06d}] rows={len(E):,} → {shard_name} | {ips} items/s | total={total_items:,}\")\n",
    "\n",
    "    # cleanup\n",
    "    del ids, texts, E, batch\n",
    "    gc.collect()\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    batch_idx += 1\n",
    "\n",
    "# Stop heartbeat if you enabled it\n",
    "stop_flag = True\n",
    "\n",
    "print(f\"Done. Batches: {batch_idx:,} | items: {total_items:,} | time: {(time.time()-t0)/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3609dd2-5e67-455f-bb47-505aad880274",
   "metadata": {},
   "source": [
    "### 7. Sanity checks: shape, dtype, norms, quick NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aef94d9-968f-4125-9de1-b89be534c82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded probe shard: shard_batch_000000.pt | shape=(200000, 384) | dtype=torch.float16\n",
      "Norm stats → mean=1.000000, std=0.000018, min=0.999914, max=1.000087, frac outside |1±0.005| = 0.000000%\n",
      "\n",
      "Nearest-neighbor sanity check:\n",
      "  Query 0: nearest neighbors → ['B008DM2LQ8', '0976640538', '1441599258', '3548241107', '1133307299']\n",
      "  Query 1: nearest neighbors → ['B007HDXO4C', '1111302731', '1424057418', '1508927618', '1932225323']\n",
      "  Query 2: nearest neighbors → ['0786477946', '1501121960', '1585426628', 'B000KZQCMK', '1952816025']\n",
      "  Query 3: nearest neighbors → ['1546879471', '0670015547', '1087719887', '1595729003', 'B007HXFBDE']\n",
      "  Query 4: nearest neighbors → ['0786019492', 'B01F0OPEB0', '0984903054', '1643136135', 'B0007ZNUUK']\n",
      "\n",
      "Sanity checks complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Load the first shard we produced ---\n",
    "files = sorted(glob.glob(os.path.join(OUT_DIR, \"shard_batch_*.pt\")))\n",
    "assert files, \"No shards found.\"\n",
    "probe = files[0]\n",
    "\n",
    "blob  = torch.load(probe, map_location=\"cpu\")\n",
    "ids   = blob[\"item_id\"]\n",
    "E     = blob[\"embeddings\"]  # [n, 384], fp16\n",
    "print(f\"Loaded probe shard: {os.path.basename(probe)} | shape={tuple(E.shape)} | dtype={E.dtype}\")\n",
    "\n",
    "# --- Check L2 norms to ensure vectors are unit-length (for cosine similarity) ---\n",
    "row_norms = E.float().norm(dim=1)\n",
    "TOL = 5e-3  # acceptable drift due to fp16 rounding\n",
    "frac_bad = float((row_norms.sub(1.0).abs() > TOL).float().mean())\n",
    "\n",
    "print(f\"Norm stats → mean={row_norms.mean():.6f}, std={row_norms.std():.6f}, \"\n",
    "      f\"min={row_norms.min():.6f}, max={row_norms.max():.6f}, \"\n",
    "      f\"frac outside |1±{TOL}| = {frac_bad:.6%}\")\n",
    "\n",
    "# --- Tiny nearest-neighbor (NN) smoke test ---\n",
    "# We’ll normalize and compute cosine similarities for a small subset\n",
    "m = min(2000, E.shape[0])     # keep it light\n",
    "Esm = F.normalize(E[:m].float(), p=2, dim=1)    # ensure re-normalized (safe)\n",
    "q   = Esm[:5]                                  # 5 queries\n",
    "S   = q @ Esm.T                                # cosine similarities\n",
    "topk = torch.topk(S, k=6, dim=1).indices       # top-1 is itself\n",
    "\n",
    "print(\"\\nNearest-neighbor sanity check:\")\n",
    "for i in range(q.shape[0]):\n",
    "    nbrs = [ids[j] for j in topk[i].tolist()[1:6]]\n",
    "    print(f\"  Query {i}: nearest neighbors → {nbrs}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del blob, E, Esm, q, S, topk, row_norms\n",
    "gc.collect()\n",
    "print(\"\\nSanity checks complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d641f73-1400-4718-80b3-b05537661fd1",
   "metadata": {},
   "source": [
    "### 8. Save run metadata summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13ad626-7131-4051-a15b-99f9bdf1d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/embeddings/items_miniLM/stats.json\n"
     ]
    }
   ],
   "source": [
    "# Quick run snapshot\n",
    "meta = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"cache_dir\": CACHE_DIR,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"arrow_batch_rows\": ARROW_BATCH_ROWS,\n",
    "    \"target_micro_bs\": TARGET_MICRO_BS,\n",
    "    \"dtype\": str(DTYPE),\n",
    "    \"norm\": NORM,\n",
    "    \"device\": DEVICE,\n",
    "    \"items_file\": ITEMS_FP,\n",
    "    \"text_col\": TEXT_COL,\n",
    "    \"out_dir\": OUT_DIR,\n",
    "    \"time_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"stats.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved:\", os.path.join(OUT_DIR, \"stats.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d561e4-5719-4ab8-9a6f-1dcf8d6d874a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis M4",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
